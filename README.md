Време за предаване на задачите:  7 календарни дни считано от датата на изпращане на заданието

Задача 1: Да се изгради ETL пайплайн, който извлича данни от JSON файл съдържащ информация за фактурите на клиентите, хостнат в GitHub хранилище на следния линк. Трансформирайте данните с помощта на Python и ги заредете в локални симулации на GCP услуги(BigQuery и Cloud Storage). Решението (кода) също трябва да бъде върнато в GitHub.
Разяснения:
•	Симулация на Cloud Storage: След като изтеглите JSON данните, вместо
директно да ги парсвате ги запишете в локална директория (напр.
landing_zone/raw_telecom_data.json). Тази директория ще симулира Cloud
Storage bucket, където първоначално се появяват суровите данни.
•	Симулация на BigQuery: Използвайте SQLite база данни, като симулация на BigQuery.
•	Наличните допълнителни пакети са: SkyShowtime, Storytel, HBO MAX, Deezer, VOYO, Bookmate, IZZi, Capital
•	Наличните роуминг пакети са: EU daily, EU weekly, EU monthly, Balkans, UK, World

Може и да не спазитете тези разяснения, а да предоставите алтернативен вариант на решение.

Задача 2: Изградете с помощта на среда, като draw.io или подобна цялостите процеси:
•	ETL процес от задача 1, как бихте го реализирали в клауд. 
•	MLOps процес с цел да подготвите данните за използването от DataScience колегите, както и контрол на версиите, автоматизиране на изпълнения и в продукционна среда и мониторинг на задачите.

При възникване на въпроси относно поставените задачи не се колебайте да ни потърсите, за да може да Ви съдействаме!
